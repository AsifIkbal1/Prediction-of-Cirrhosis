{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60893,"databundleVersionId":7000181,"sourceType":"competition"},{"sourceId":6724823,"sourceType":"datasetVersion","datasetId":3873965}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### PROBLEM STATEMENT: Cirrhosis Patient Survival Prediction\n\nDevelop a machine learning model using 19 clinical features to predict the survival state of patients with liver cirrhosis. The survival states, categorized as 0 = D (death), 1 = C (censored), and 2 = CL (censored due to liver transplantation), will be inferred from data sourced from a Mayo Clinic study on primary biliary cirrhosis (PBC) of the liver conducted between 1974 and 1984.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1.LOAD DATA ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:47.974433Z","iopub.execute_input":"2024-01-01T03:15:47.974814Z","iopub.status.idle":"2024-01-01T03:15:49.495499Z","shell.execute_reply.started":"2024-01-01T03:15:47.974787Z","shell.execute_reply":"2024-01-01T03:15:49.494451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/playground-series-s3e26/train.csv')\ntest=pd.read_csv('/kaggle/input/playground-series-s3e26/test.csv')\nsubmission=pd.read_csv('/kaggle/input/playground-series-s3e26/sample_submission.csv')\noriginal = pd.read_csv('../input/cirrhosis-patient-survival-prediction/cirrhosis.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.497762Z","iopub.execute_input":"2024-01-01T03:15:49.49826Z","iopub.status.idle":"2024-01-01T03:15:49.59648Z","shell.execute_reply.started":"2024-01-01T03:15:49.498232Z","shell.execute_reply":"2024-01-01T03:15:49.595312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.598303Z","iopub.execute_input":"2024-01-01T03:15:49.599084Z","iopub.status.idle":"2024-01-01T03:15:49.606196Z","shell.execute_reply.started":"2024-01-01T03:15:49.599059Z","shell.execute_reply":"2024-01-01T03:15:49.605115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.608265Z","iopub.execute_input":"2024-01-01T03:15:49.608614Z","iopub.status.idle":"2024-01-01T03:15:49.614054Z","shell.execute_reply.started":"2024-01-01T03:15:49.608555Z","shell.execute_reply":"2024-01-01T03:15:49.613459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=[\"id\"],inplace=True)\ntest.drop(columns=[\"id\"],inplace=True)\noriginal.drop(columns=[\"ID\"],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.616758Z","iopub.execute_input":"2024-01-01T03:15:49.617809Z","iopub.status.idle":"2024-01-01T03:15:49.635708Z","shell.execute_reply.started":"2024-01-01T03:15:49.617781Z","shell.execute_reply":"2024-01-01T03:15:49.63472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy=train.copy()\ntest_copy=test.copy()\noriginal_copy=original.copy()\noriginal[\"original\"]=1\n\ntrain[\"original\"]=0\ntest[\"original\"]=0\n\n\n\ntarget='Status'\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.63725Z","iopub.execute_input":"2024-01-01T03:15:49.637809Z","iopub.status.idle":"2024-01-01T03:15:49.724687Z","shell.execute_reply.started":"2024-01-01T03:15:49.637781Z","shell.execute_reply":"2024-01-01T03:15:49.723584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. DATA CLEANING\n## 2.1 MISSING VALUE CHECKS","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom prettytable import PrettyTable  # Use the PrettyTable library to create a visually appealing table\nimport numpy as np  # Use the numpy library for numerical operations\n\n# Remove missing values from the training dataset\ntrain_copy = train_copy.dropna()\n\n# Create a PrettyTable object to store information about the data\ntable = PrettyTable()\n\n# Set names for the columns in the table\ntable.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %', 'Original Missing%']\n\n# Iterate through each column in the 'train_copy' DataFrame\nfor column in train_copy.columns:\n    # Get the data type of the current column\n    data_type = str(train_copy[column].dtype)\n    \n    # Calculate the percentage of missing values in the 'train_copy' dataset for the current column\n    non_null_count_train = np.round(100 - train_copy[column].count() / train_copy.shape[0] * 100, 1)\n    \n    # Check if the current column is not the target column\n    if column != target:\n        # Calculate the percentage of missing values in the 'test_copy' dataset for the current column (if it is not the target column)\n        non_null_count_test = np.round(100 - test_copy[column].count() / test_copy.shape[0] * 100, 1)\n    else:\n        # If the current column is the target column, set 'NA' for the missing value percentage in testing\n        non_null_count_test = \"NA\"\n    \n    # Calculate the percentage of missing values in the 'original_copy' dataset for the current column\n    non_null_count_orig = np.round(100 - original_copy[column].count() / original_copy.shape[0] * 100, 1)\n    \n    # Add a row to the table with information about the current column\n    table.add_row([column, data_type, non_null_count_train, non_null_count_test, non_null_count_orig])\n\n# Print the table\nprint(table)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.72616Z","iopub.execute_input":"2024-01-01T03:15:49.726512Z","iopub.status.idle":"2024-01-01T03:15:49.796118Z","shell.execute_reply.started":"2024-01-01T03:15:49.726486Z","shell.execute_reply":"2024-01-01T03:15:49.795326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(original_copy, color=  (0.8, 0.56, 0.65))\nplt.title(\"Original Data Missing Value Matrix\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:49.797389Z","iopub.execute_input":"2024-01-01T03:15:49.797895Z","iopub.status.idle":"2024-01-01T03:15:50.428368Z","shell.execute_reply.started":"2024-01-01T03:15:49.797866Z","shell.execute_reply":"2024-01-01T03:15:50.427824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data loss on the original set was quite large, >25% in many columns, so we had to delete it.\n\nAfter I checked the file, the data unexpectedly had no rows.","metadata":{"execution":{"iopub.status.busy":"2023-12-29T12:29:18.596165Z","iopub.execute_input":"2023-12-29T12:29:18.596621Z","iopub.status.idle":"2023-12-29T12:29:18.605254Z","shell.execute_reply.started":"2023-12-29T12:29:18.596586Z","shell.execute_reply":"2023-12-29T12:29:18.603665Z"}}},{"cell_type":"code","source":"train_copy.duplicated().sum()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:50.429139Z","iopub.execute_input":"2024-01-01T03:15:50.429352Z","iopub.status.idle":"2024-01-01T03:15:50.444511Z","shell.execute_reply.started":"2024-01-01T03:15:50.429331Z","shell.execute_reply":"2024-01-01T03:15:50.443526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_copy = original_copy.dropna()\noriginal_copy.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:50.448513Z","iopub.execute_input":"2024-01-01T03:15:50.448796Z","iopub.status.idle":"2024-01-01T03:15:50.456794Z","shell.execute_reply.started":"2024-01-01T03:15:50.448774Z","shell.execute_reply":"2024-01-01T03:15:50.455956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy=pd.concat([train_copy,original_copy],axis=0)\ntrain_copy.reset_index(inplace=True,drop=True)\ntrain_copy.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:50.458333Z","iopub.execute_input":"2024-01-01T03:15:50.458604Z","iopub.status.idle":"2024-01-01T03:15:50.472305Z","shell.execute_reply.started":"2024-01-01T03:15:50.45856Z","shell.execute_reply":"2024-01-01T03:15:50.471096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. EXPLORATORY DATA\n## 3.1 Dataset info","metadata":{}},{"cell_type":"markdown","source":"Features in the dataset are: N_Days', 'Drug', 'Age', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Bilirubin', 'Cholesterol', 'Albumin ', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage', 'Status',\nN_Days: Number of days (Number of days), can link to monitoring or treatment period.\n\nDrug: A drug used in treatment or research.\n\nAge: Patient's age.\n\nGender: Patient's gender (Male/Female).\n\nAscites: The presence of fluid in the abdomen.\n\nEnlarged liver: Liver exhaustion.\n\nSpiders: The presence of small arteries growing in the skin.\n\nEdema: Swelling and pain due to accumulation of excess substances.\n\nBilirubin: Blood bilirubin index, an indicator of liver health.\n\nCholesterol: The amount of cholesterol in the blood.\n\nAlbumin: A blood protein that plays an important role in maintaining glue performance and water retention function.\n\nCopper: The amount of copper in the blood.\n\nAlk_Phos: Alkaline Phosphatase enzyme in the blood.\n\nSGOT: Enzyme Aspartate Aminotransferase in the blood.\n\nTrygliceride: The amount of triglyceride in the blood.\n\nPlatelets: The number of platelets (platelets) in the blood.\n\nProtrombin: blood clotting time.\n\nStage: The stage or danger level of the disease.\n\nState: Health state (can be target variable in analysis).","metadata":{}},{"cell_type":"code","source":"train_copy.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:50.473684Z","iopub.execute_input":"2024-01-01T03:15:50.474104Z","iopub.status.idle":"2024-01-01T03:15:50.496485Z","shell.execute_reply.started":"2024-01-01T03:15:50.47407Z","shell.execute_reply":"2024-01-01T03:15:50.495626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Numerical Feature Distributions","metadata":{}},{"cell_type":"code","source":"# Filter numerical columns\ncont_cols = [f for f in train_copy.columns if train_copy[f].dtype != 'O' and train_copy[f].nunique() > 2 and f != target]\n\n# Number of rows in the subplot grid\nn_rows = len(cont_cols)\n\n# Create figure and axes\nfig, axs = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\nfig.tight_layout()\n\n# Set color palette\nsns.set_palette([(0.8, 0.56, 0.65), 'crimson',  (0.99, 0.8, 0.3)])\n\n# Iterate through each numerical column and plot violin plots\nfor i, col in enumerate(cont_cols):\n    # Use flat to flatten axs into a 1D array\n    sns.violinplot(x=target, y=col, data=train_copy, ax=axs.flat[i])\n    axs.flat[i].set_title(f'{col.title()} Distribution by Target (Train)', fontsize=14)\n    axs.flat[i].set_xlabel('Outcome', fontsize=12)\n    axs.flat[i].set_ylabel(col.title(), fontsize=12)\n    sns.despine()\n\n# Remove excess plots\nfor ax_row in axs:\n    for ax in ax_row:\n        if not ax.has_data():\n            fig.delaxes(ax)    \n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:50.497677Z","iopub.execute_input":"2024-01-01T03:15:50.498473Z","iopub.status.idle":"2024-01-01T03:15:53.746308Z","shell.execute_reply.started":"2024-01-01T03:15:50.49844Z","shell.execute_reply":"2024-01-01T03:15:53.745316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nmask = np.triu(np.ones_like(train_copy[cont_cols].corr(), dtype=bool))\nsns.heatmap(train_copy[cont_cols].corr(), annot=True, mask=mask)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:53.747459Z","iopub.execute_input":"2024-01-01T03:15:53.74774Z","iopub.status.idle":"2024-01-01T03:15:54.189107Z","shell.execute_reply.started":"2024-01-01T03:15:53.747717Z","shell.execute_reply":"2024-01-01T03:15:54.188065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Categorical Features Analysis\nPart 1:\n\n(train_copy[f].dtype != 'O' and train_copy[f].nunique() / train_copy.shape[0] < 0.025):\nEliminate columns that have a data type other than objects ('O') and whose number of unique values ​​is less than 2.5% of the number of samples in the DataFrame. This can help eliminate columns that have little variation and don't provide much information.\n\nPart 2:\n\n(train_copy[f].dtype == 'O' and f != target):\nRemove columns that have data type of object ('O') and are not target columns. This is to avoid analyzing columns that are not target columns and can be considered separately.","metadata":{}},{"cell_type":"code","source":"# Filter taxonomy columns based on conditions\ncat_cols = [f for f in train_copy.columns if\n            ((train_copy[f].dtype != 'O' and train_copy[f].nunique() / train_copy.shape[0] < 0.025) or\n             (train_copy[f].dtype == 'O' and f != target))]\n\n# Define color palette\ncustom_palette = [(0.8, 0.56, 0.65), 'crimson', (0.99, 0.8, 0.3)]\n\n# Draw a Stacked Bar chart for each classification column\nfor col in cat_cols:\n    # Create contingency table\n    contingency_table = pd.crosstab(train_copy[col], train_copy[target], normalize='index')\n    \n    # Set style for chart\n    sns.set(style=\"whitegrid\")\n    \n    # Draw the Stacked Bar chart\n    contingency_table.plot(kind=\"bar\", stacked=True, color=custom_palette, figsize=(20, 4))\n    \n    # Set up title and axis labels\n    plt.title(f\"Percentage Distribution of {target} across {col}\")\n    plt.xlabel(col)\n    plt.ylabel(\"Percentage\")\n    \n    # Show caption\n    plt.legend(title=target)\n    \n    # Show the chart\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:15:54.190506Z","iopub.execute_input":"2024-01-01T03:15:54.190798Z","iopub.status.idle":"2024-01-01T03:16:05.898293Z","shell.execute_reply.started":"2024-01-01T03:15:54.190776Z","shell.execute_reply":"2024-01-01T03:16:05.897289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Train model\n","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n\n    df[\"Drug\"] = df[\"Drug\"].map({\"Placebo\": 1, \"D-penicillamine\": 0})\n    df[\"Sex\"] = df[\"Sex\"].map({\"M\": 1, \"F\": 0})\n    \n    for col in [\"Ascites\", \"Hepatomegaly\", \"Spiders\"]:\n        df[col] = df[col].map({\"N\": 0, \"Y\": 1})\n        \n    df[\"Edema\"] = df[\"Edema\"].map({\"N\": 0, \"S\": 1, \"Y\": 2})\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:05.899918Z","iopub.execute_input":"2024-01-01T03:16:05.900255Z","iopub.status.idle":"2024-01-01T03:16:05.90597Z","shell.execute_reply.started":"2024-01-01T03:16:05.900231Z","shell.execute_reply":"2024-01-01T03:16:05.904946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy[target] = train_copy[target].map({\"D\": 0, \"CL\": 1, \"C\": 2})","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:05.907388Z","iopub.execute_input":"2024-01-01T03:16:05.9082Z","iopub.status.idle":"2024-01-01T03:16:05.929369Z","shell.execute_reply.started":"2024-01-01T03:16:05.908162Z","shell.execute_reply":"2024-01-01T03:16:05.928385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess(train_copy)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:05.930872Z","iopub.execute_input":"2024-01-01T03:16:05.931145Z","iopub.status.idle":"2024-01-01T03:16:05.968798Z","shell.execute_reply.started":"2024-01-01T03:16:05.931121Z","shell.execute_reply":"2024-01-01T03:16:05.967922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Create X and y from the 'train_copy' data\nX = train_copy.drop(columns=[target])  \ny = train_copy[target]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Train the XGBoost model\nxgb_model = XGBClassifier()\nxgb_model.fit(X_train, y_train)\n\n# Train the LightGBM model\nlgb_model = LGBMClassifier()\nlgb_model.fit(X_train, y_train)\n\n# Train the CatBoost model\ncat_model = CatBoostClassifier(verbose=False)\ncat_model.fit(X_train, y_train)\n\n# Predict on the validation set to calculate log_loss\nxgb_preds = xgb_model.predict_proba(X_val)\nlgb_preds = lgb_model.predict_proba(X_val)\ncat_preds = cat_model.predict_proba(X_val)\n\n# Calculate log_loss for each model\nxgb_score = log_loss(y_val, xgb_preds)\nlgb_score = log_loss(y_val, lgb_preds)\ncat_score = log_loss(y_val, cat_preds)\n\n# Display the results\nprint(f\"XGBoost log_loss: {xgb_score:.5f}\")\nprint(f\"LightGBM log_loss: {lgb_score:.5f}\")\nprint(f\"CatBoost log_loss: {cat_score:.5f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:05.970562Z","iopub.execute_input":"2024-01-01T03:16:05.970934Z","iopub.status.idle":"2024-01-01T03:16:15.230868Z","shell.execute_reply.started":"2024-01-01T03:16:05.970903Z","shell.execute_reply":"2024-01-01T03:16:15.229845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame to store feature importance\nimportance_df = pd.DataFrame(index=X_train.columns)\n\n# Calculate feature importance for the XGBoost model\nxgb_importance = xgb_model.feature_importances_\nimportance_df['XGBoost'] = xgb_importance\n\n# Calculate feature importance for the LightGBM model\nlgb_importance = lgb_model.feature_importances_\nimportance_df['LightGBM'] = lgb_importance\n\n# Calculate feature importance for the CatBoost model\ncat_importance = cat_model.feature_importances_\nimportance_df['CatBoost'] = cat_importance\n\n# Sort in descending order by XGBoost model importance\nimportance_df_sorted_xgb = importance_df.sort_values(by='XGBoost', ascending=False)\n\n# Print XGBoost model feature importance\nprint(\"XGBoost Feature Importance:\")\nprint(importance_df_sorted_xgb['XGBoost'])\n\n# Sort in descending order by LightGBM model importance\nimportance_df_sorted_lgb = importance_df.sort_values(by='LightGBM', ascending=False)\n\n# Print LightGBM model feature importance\nprint(\"\\nLightGBM Feature Importance:\")\nprint(importance_df_sorted_lgb['LightGBM'])\n\n# Sort in descending order by CatBoost model importance\nimportance_df_sorted_cat = importance_df.sort_values(by='CatBoost', ascending=False)\n\n# Print CatBoost model feature importance\nprint(\"\\nCatBoost Feature Importance:\")\nprint(importance_df_sorted_cat['CatBoost'])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:15.231987Z","iopub.execute_input":"2024-01-01T03:16:15.232423Z","iopub.status.idle":"2024-01-01T03:16:15.247291Z","shell.execute_reply.started":"2024-01-01T03:16:15.232398Z","shell.execute_reply":"2024-01-01T03:16:15.246612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw a clustermap for the XGBoost model feature importance\nsns.clustermap(importance_df_sorted_xgb[['XGBoost']], cmap='coolwarm', method='average', col_cluster=False)\nplt.title('XGBoost Feature Importance Clustermap')\nplt.show()\n\n# Draw a clustermap for the LightGBM model feature importance\nsns.clustermap(importance_df_sorted_lgb[['LightGBM']], cmap='coolwarm', method='average', col_cluster=False)\nplt.title('LightGBM Feature Importance Clustermap')\nplt.show()\n\n# Draw a clustermap for the CatBoost model feature importance\nsns.clustermap(importance_df_sorted_cat[['CatBoost']], cmap='coolwarm', method='average', col_cluster=False)\nplt.title('CatBoost Feature Importance Clustermap')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:15.248624Z","iopub.execute_input":"2024-01-01T03:16:15.248874Z","iopub.status.idle":"2024-01-01T03:16:16.91801Z","shell.execute_reply.started":"2024-01-01T03:16:15.248853Z","shell.execute_reply":"2024-01-01T03:16:16.916948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns Sex , Ascites , Hepatomegaly , Spiders , Edama , Stage from the chart show information gaps but when applied in training do not carry much information and other than Bilirubin, the columns I guess bring a lot The information does not help the algorithm as much as other feature columns, and those columns often belong to the classification group.","metadata":{}},{"cell_type":"markdown","source":"# 5.Feature Engineering\n","metadata":{}},{"cell_type":"markdown","source":"\nTo make the other column features more useful, we link the features through the newly created column features","metadata":{"execution":{"iopub.status.busy":"2023-12-30T21:49:32.20585Z","iopub.execute_input":"2023-12-30T21:49:32.206257Z","iopub.status.idle":"2023-12-30T21:49:32.246617Z","shell.execute_reply.started":"2023-12-30T21:49:32.206225Z","shell.execute_reply":"2023-12-30T21:49:32.245154Z"}}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\nclass DiagnosisDateTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X['Diagnosis_Date'] = X['Age'] - X['N_Days']\n        return X\nclass AlbuminLevelTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['Albumin_Level'] = np.where((X['Albumin'] < 3.4), 1, np.where((X['Albumin'] > 5.4), 2, 0))\n        return X\nclass CopperRiskTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['Copper_Risk'] = np.where((X['Copper'] >= 62) & (X['Copper'] <= 140), 1, 0)\n        return X\nclass BilirubinNormalTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['Bilirubin_Normal'] = np.where((X['Bilirubin'] >= 0.1) & (X['Bilirubin'] <= 1.2), 1, 0)\n        return X\nclass NormalSGOTTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['Normal_SGOT'] = np.where((X['SGOT'] >= 8) & (X['SGOT'] <= 45), 1, 0)\n        return X\nclass NormalProthrombinTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['Normal_Prothrombin'] = np.where((X['Prothrombin'] >= 9.4) & (X['Prothrombin'] <= 12.5), 1, 0)\n        return X\nclass NormalAlkPhosTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['Normal_Alk_Phos'] = np.where((X['Alk_Phos'] >= 44) & (X['Alk_Phos'] <= 147), 1, 0)\n        return X\nclass PlateletsTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Label as 1 (Low) when Platelets are between 150 and 400, otherwise 0\n        X['Low_Platelets'] = np.where((X['Platelets'] >= 150) & (X['Platelets'] <= 400), 1, 0)\n        return X\nclass CholesterolTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Label as 0 (Normal) or 1 (High)\n        X['Cholesterol_Level'] = np.where(X['Cholesterol'] > 200, 1, 0)\n        return X\n\nclass TrygliceridesTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Label as 0 (Normal) or 1 (High)\n        X['Tryglicerides_Level'] = np.where(X['Tryglicerides'] > 150, 1, 0)\n        return X\n\nclass PredictStatusTransformer(BaseEstimator, TransformerMixin):\n    '''Nếu thời gian điều trị ít hơn giai đoạn của bệnh nhân \n    thì có thể do họ dự đoán được kết quả..\n    '''\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X['N_Days_per_Stage'] = np.where(X['N_Days'] / X['Stage'] > 300, 1, 0)\n        return X\nclass AgeGroupsTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Older people might be hit harder (interaction) \n        by health issues. Also can cover lifestyle influences, i.e.\n        alcohol consumption etc.\"\"\"\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        # Use years from above, min=26, max=78\n        X['Age_Group'] = pd.cut(round(X['Age'] / 365.25).astype(\"int16\"), \n                                bins=[19, 29, 49, 64, 99], \n                                labels = [0, 1, 2, 3]\n                               ).astype('int16')\n        return X\nclass BilirubinAlbuminTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X['Bilirubin_Albumin'] = X['Bilirubin'] / X['Albumin']\n        return X\nclass SymptomScoreTransformer(BaseEstimator, TransformerMixin):\n    # From data set explanations above let's add all the \"bad\" symptoms\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        symptom_columns = [\"Drug\",'Ascites', 'Hepatomegaly', \n                           'Spiders', 'Edema']\n        X['Symptom_Score'] = X[symptom_columns].sum(axis=1)\n        return X\nclass APRITransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X['APRI'] = X['SGOT'] / (X['Platelets'] * 100 + 0.0000001)\n        return X\nclass DrugEffectivenessTransformer(BaseEstimator, TransformerMixin):\n    # Placeholder concept, assuming 'Bilirubin' improvement is a measure of effectiveness\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X['Drug_Effectiveness'] = X['Drug'] * X['Bilirubin']\n        return X\nclass LiverFunctionIndexTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        liver_columns = ['Bilirubin', 'Albumin', 'Alk_Phos', 'SGOT']\n        X['Liver_Function_Index'] = X[liver_columns].mean(axis=1)\n        return X\nclass BilirubinAlkRatioTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Tính toán tỉ lệ Bilirubin / Alk_Phos\n        X['Bilirubin_Alk_Ratio'] = X['Bilirubin'] * (X['Alk_Phos'] + 0.0000001)\n        return X\nclass CholesterolTriglyceridesRatioTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Tính toán tỉ lệ cholesterol / triglycerides\n        X['Cholesterol_Tryglicerides_Ratio'] = X['Cholesterol'] / X['Tryglicerides']\n        return X\nclass TimeFeaturesTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X['Diag_Year'] = (X['N_Days'] / 365).astype(int)\n        X['Diag_Month'] = ((X['N_Days'] % 365) / 30).astype(int)\n        return X\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:16.919354Z","iopub.execute_input":"2024-01-01T03:16:16.919668Z","iopub.status.idle":"2024-01-01T03:16:16.944428Z","shell.execute_reply.started":"2024-01-01T03:16:16.919644Z","shell.execute_reply":"2024-01-01T03:16:16.943218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Add all transformers to the pipeline\npipeline = Pipeline([\n    #('predict_status', PredictStatusTransformer()),\n    ('age_groups', AgeGroupsTransformer()),\n    ('time_features', TimeFeaturesTransformer()),\n    #('diagnosis_date', DiagnosisDateTransformer()),  \n    #('bilirubin_albumin', BilirubinAlbuminTransformer()),\n    #('symptom_score', SymptomScoreTransformer()),\n    #('apri', APRITransformer()),\n    #('drug_effectiveness', DrugEffectivenessTransformer()),\n    ('albumin_level', AlbuminLevelTransformer()),    \n    ('copper_risk', CopperRiskTransformer()),  \n    ('bilirubin_normal', BilirubinNormalTransformer()),\n    #('liver_function_index', LiverFunctionIndexTransformer()),\n    #('cholesterol_triglycerides_ratio', CholesterolTriglyceridesRatioTransformer()),\n    ('normal_sgot', NormalSGOTTransformer()),\n    ('normal_prothrombin', NormalProthrombinTransformer()),\n    ('normal_alk_phos', NormalAlkPhosTransformer()),\n    #('bilirubin_alk_ratio', BilirubinAlkRatioTransformer()),\n    #('cholesterol_level', CholesterolTransformer()),\n    #('tryglicerides_level', TrygliceridesTransformer()),\n    #('platelets_level', PlateletsTransformer()),\n    \n])\n# Apply the pipeline to the DataFrame df\ntrain_copy_transformed = pipeline.fit_transform(train_copy)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:16.946198Z","iopub.execute_input":"2024-01-01T03:16:16.946617Z","iopub.status.idle":"2024-01-01T03:16:17.444937Z","shell.execute_reply.started":"2024-01-01T03:16:16.946564Z","shell.execute_reply":"2024-01-01T03:16:17.442059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_cols = [f for f in train_copy.columns if train_copy[f].dtype != 'O' and train_copy[f].nunique() > 2 and f != target]\ndef high_corr_drop(df, cols):\n    df = df.copy()\n    # Select only numeric columns for correlation calculation\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    correlation = df[cols].corr().abs()\n    mask = np.triu(np.ones_like(correlation).astype(bool), k=1)\n    drop_list = correlation[(correlation.where(mask) > 0.91).sum() > 0].index.tolist()\n    df = df.drop(drop_list, axis=1)\n        \n    return df\n#train_copy = high_corr_drop(train_copy,cont_cols)\n#test_copy = high_corr_drop(test_copy,cont_cols)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.446305Z","iopub.status.idle":"2024-01-01T03:16:17.446765Z","shell.execute_reply.started":"2024-01-01T03:16:17.446524Z","shell.execute_reply":"2024-01-01T03:16:17.446545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncont_cols = [f for f in train_copy.columns if train_copy[f].dtype != 'O' and train_copy[f].nunique() > 2]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.448408Z","iopub.status.idle":"2024-01-01T03:16:17.44885Z","shell.execute_reply.started":"2024-01-01T03:16:17.448631Z","shell.execute_reply":"2024-01-01T03:16:17.448652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nmask = np.triu(np.ones_like(train_copy[cont_cols].corr(), dtype=bool))\nsns.heatmap(train_copy[cont_cols].corr(), annot=True, mask=mask)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.450096Z","iopub.status.idle":"2024-01-01T03:16:17.450508Z","shell.execute_reply.started":"2024-01-01T03:16:17.450308Z","shell.execute_reply":"2024-01-01T03:16:17.450328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.Refine the Model","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import log_loss\n\n# Create X and y from the train_copy data\nX = train_copy.drop(columns=[target])\ny = train_copy[target]\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Define the objective function for XGBoost\ndef objective_xgb(trial):\n    params = {\n        'booster': trial.suggest_categorical('booster', ['gbtree']),\n        'max_depth': trial.suggest_int('max_depth', 3, 7),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n        'subsample': trial.suggest_loguniform('subsample', 0.3, 0.9),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n        \"seed\": trial.suggest_categorical('seed', [42]),\n        'objective': trial.suggest_categorical('objective', ['multi:softmax']),\n        'num_class': 3  # Replace 3 with the number of classes in the target variable\n    }\n\n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    # Predict on the validation set to calculate log_loss\n    preds = model.predict_proba(X_val)\n    score = xgb_score = log_loss(y_val, preds)\n    return score\n\n# Define the objective function for LightGBM\ndef objective_lgb(trial):\n    params = {\n        'objective': 'multiclass',\n        'num_class': 3,  # Replace 3 with the number of classes in the target variable\n        'boosting_type': 'gbdt',\n        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 1000),\n        'max_depth': trial.suggest_int('max_depth', 5, 15),\n        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-5, 1e2),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n        'random_state': 42\n    }\n\n    model = LGBMClassifier(**params)\n    model.fit(X_train, y_train)\n\n    # Predict on the validation set to calculate log_loss\n    preds = model.predict_proba(X_val)\n    score = log_loss(y_val, preds)\n    return score\n\n# Define the objective function for CatBoost\ndef objective_cat(trial):\n    params = {\n        'iterations': trial.suggest_int('iterations', 100, 1000),\n        'depth': trial.suggest_int('depth', 4, 8),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1.5),\n        'random_strength': trial.suggest_float('random_strength', 0.01, 3),\n        'max_bin': trial.suggest_int('max_bin', 2, 255),\n        'od_wait': trial.suggest_int('od_wait', 1, 50),\n        'od_type': trial.suggest_categorical('od_type', ['Iter']),\n        'one_hot_max_size': trial.suggest_int('one_hot_max_size', 40, 70),\n        'custom_metric': 'MultiClass',  # Change here\n        'random_state': 42,\n        'verbose': False\n    }\n\n    model = CatBoostClassifier(**params)\n    model.fit(X_train, y_train)\n\n    # Predict on the validation set to calculate log_loss\n    preds = model.predict_proba(X_val)\n    score = log_loss(y_val, preds)\n    return score\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.452357Z","iopub.status.idle":"2024-01-01T03:16:17.453157Z","shell.execute_reply.started":"2024-01-01T03:16:17.452912Z","shell.execute_reply":"2024-01-01T03:16:17.452936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_xgb = optuna.create_study(direction='minimize')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy_xgb.optimize(objective_xgb, n_trials=50,show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.454517Z","iopub.status.idle":"2024-01-01T03:16:17.455617Z","shell.execute_reply.started":"2024-01-01T03:16:17.455371Z","shell.execute_reply":"2024-01-01T03:16:17.455393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best parameters', study_xgb.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.456707Z","iopub.status.idle":"2024-01-01T03:16:17.457517Z","shell.execute_reply.started":"2024-01-01T03:16:17.457268Z","shell.execute_reply":"2024-01-01T03:16:17.457295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best-performing model\nbest_params_xgb = study_xgb.best_params\nbest_model_xgb = XGBClassifier(**best_params_xgb)\nbest_model_xgb.fit(X_train, y_train)\n\n# Create a DataFrame to store feature importances and print\nimportances_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': best_model_xgb.feature_importances_\n})\n\n# Sort and print feature importances\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)\nprint(importances_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.458968Z","iopub.status.idle":"2024-01-01T03:16:17.459523Z","shell.execute_reply.started":"2024-01-01T03:16:17.459192Z","shell.execute_reply":"2024-01-01T03:16:17.459211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_lgb = optuna.create_study(direction='minimize')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy_lgb.optimize(objective_lgb, n_trials=50,show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.460765Z","iopub.status.idle":"2024-01-01T03:16:17.461212Z","shell.execute_reply.started":"2024-01-01T03:16:17.460982Z","shell.execute_reply":"2024-01-01T03:16:17.461004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best parameters', study_lgb.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.462725Z","iopub.status.idle":"2024-01-01T03:16:17.463139Z","shell.execute_reply.started":"2024-01-01T03:16:17.462931Z","shell.execute_reply":"2024-01-01T03:16:17.46295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best-performing model from the LGB study\nbest_params_lgb = study_lgb.best_params\nbest_model_lgb = LGBMClassifier(**best_params_lgb)\nbest_model_lgb.fit(X_train, y_train)\n\n# Create a DataFrame to store feature importances and print\nimportances_df_lgb = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': best_model_lgb.feature_importances_\n})\n\n# Sort and print feature importances\nimportances_df_lgb = importances_df_lgb.sort_values(by='Importance', ascending=False)\nprint(importances_df_lgb)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.464715Z","iopub.status.idle":"2024-01-01T03:16:17.465144Z","shell.execute_reply.started":"2024-01-01T03:16:17.464926Z","shell.execute_reply":"2024-01-01T03:16:17.464947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study_cat = optuna.create_study(direction='minimize')\n#optuna.logging.set_verbosity(optuna.logging.WARNING)\n#study_cat.optimize(objective_cat, n_trials=50,show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.466821Z","iopub.status.idle":"2024-01-01T03:16:17.467227Z","shell.execute_reply.started":"2024-01-01T03:16:17.467017Z","shell.execute_reply":"2024-01-01T03:16:17.467045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('Best parameters', study_cat.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.468297Z","iopub.status.idle":"2024-01-01T03:16:17.468714Z","shell.execute_reply.started":"2024-01-01T03:16:17.468491Z","shell.execute_reply":"2024-01-01T03:16:17.46851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best-performing model from the CatBoost study\n\n#best_params_cat = study_cat.best_params\n#best_model_cat = CatBoostClassifier(**best_params_cat, verbose=False)\n#best_model_cat.fit(X_train, y_train)\n\n# Create a DataFrame to store feature importances and print\n#importances_df_cat = pd.DataFrame({\n#    'Feature': X_train.columns,\n#    'Importance': best_model_cat.feature_importances_\n#})\n\n# Sort and print feature importances\n#importances_df_cat = importances_df_cat.sort_values(by='Importance', ascending=False)\n#print(importances_df_cat)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.470255Z","iopub.status.idle":"2024-01-01T03:16:17.470753Z","shell.execute_reply.started":"2024-01-01T03:16:17.470452Z","shell.execute_reply":"2024-01-01T03:16:17.470472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7.Predict and submit","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import log_loss\n\n# Use the best model from each algorithm\nvoting_model = VotingClassifier(estimators=[\n    #('catboost', best_model_cat),\n    ('lgbm', best_model_lgb),\n    ('xgboost', best_model_xgb)\n], voting='soft')  # 'soft' to use probability estimates\n\n# Train the ensemble model\nvoting_model.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred = voting_model.predict_proba(X_val)\n\n# Evaluate log loss\nlogloss = log_loss(y_val, y_pred)\nprint(f'Log Loss of the ensemble model: {logloss}')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.47246Z","iopub.status.idle":"2024-01-01T03:16:17.472904Z","shell.execute_reply.started":"2024-01-01T03:16:17.472689Z","shell.execute_reply":"2024-01-01T03:16:17.472708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess(test_copy)\npipeline.fit_transform(test_copy)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.474451Z","iopub.status.idle":"2024-01-01T03:16:17.47487Z","shell.execute_reply.started":"2024-01-01T03:16:17.474668Z","shell.execute_reply":"2024-01-01T03:16:17.474687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = best_model_lgb.predict_proba(test_copy)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.475753Z","iopub.status.idle":"2024-01-01T03:16:17.476143Z","shell.execute_reply.started":"2024-01-01T03:16:17.475945Z","shell.execute_reply":"2024-01-01T03:16:17.475964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    'id': submission['id'],\n    'Status_C': test_predictions[:, 2],\n    'Status_CL': test_predictions[:, 1],\n    'Status_D': test_predictions[:, 0]\n})\n\n\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.47894Z","iopub.status.idle":"2024-01-01T03:16:17.47984Z","shell.execute_reply.started":"2024-01-01T03:16:17.479627Z","shell.execute_reply":"2024-01-01T03:16:17.479648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:16:17.480834Z","iopub.status.idle":"2024-01-01T03:16:17.48189Z","shell.execute_reply.started":"2024-01-01T03:16:17.481655Z","shell.execute_reply":"2024-01-01T03:16:17.481678Z"},"trusted":true},"execution_count":null,"outputs":[]}]}